---
title: ' How Many Boys is Too Many?

'
author: Michael Johnson
date: '2019-1-12
'
slug: How-many-is-too-many
categories:
  - simulation
  - data science
tags:
  - wikipedia
  - python
  - SpaCy
  - NLP
  - pandas
type: ''
subtitle: 'Statistical Evaluation of Having Lots of Boys'
image: 'kids.jpg'
math: true
---
Anyone who has multiple boys in a row at some point asks themselves a question: did I just get 'lucky' or are we only able to have boys? As you can see, we certainly asked that question...
![kids](/img/kids.jpg)

When we had our third boy in a row I started to wonder: how many children do you have to have of a given sex before you can conclude that there is something other than random chance influencing to the outcome?   

We can cast this problem as a classic coin flip probability, and use a statistical measure (the p-value) to make a judgment on if there could be something going on beyond chance. Then we can conclusively and statistically answer the question: **How many is too many?**

Turns out the answer is 5. If you have 5 children in a row, and they are all boys, you can statistically conclude there is something else at play. Or is it 8? Or 9? Perhaps you'll have to read and decide for yourself...

# Probability Distributions and The Null Hypothesis
Hypothesis testing is one of the most important and foundational concepts to Data Science. Almost every problem involving data can be cast into this framework:

1. Develop a **null hypothesis**.
  * There is an equal probability of having boys and girls.
3. Identify the **outcome** you'd like to test (or have observed).
  * 5 children, all boys
2. **Calculate/simulate** the probability of the outcome given the null hypothesis.
  * If there is an equal probability of having boys and girls, how likely is it that one family with 5 children will have 5 girls?
3. **Compare** the answer you got above with a predetermined criteria (the p value) to decide if you can reject the null hypothesis. (typically taken to be 0.05, or 5%)

The null hypothesis is of assuming **innocent until proven guilty**. In the case of having children, the null hypothesis would assume that the probability of having boys and girls is equal (you are trying to prove that this is is wrong, so you start with the assumption that it is correct). If you are trying to evaluate the fairness of a coin, the null hypothesis  suggests the coin is fair. If you are trying to determine if the distribution of defects across a lot of semiconductors is process driven, you assume that defects are equally likely to occur anywhere.


If you can successfully show (through calculation or simulation) that your observed outcome is highly unlikely under the null hypothesis, you can reject the null hypothesis. This is typically done with things that are easily quantifiable (like coin flips, die rolls, defects), but we follow a similar process with non-quantifiable things. For example, I might judge that the probability that a certain toy ends up under my son's bed given that he didn't take it and stash it there is exceptionally low. In this case, the null hypothesis (he is innocent) is rejected because of the evidence (found charger, charger doesn't have legs).

If we did 100 experiments, how many times might we observe our outcome based on random chance?

When it is applied to a quantifiable problem, we still have to determine a threshold for rejecting the null hypothesis. Do we say that if its less than 50% likely we can reject? How about 20%? How about 10%? This number is typically taken to be .05, or 5 out of 100. As you will se below, there is some contention about this number, and rightly so.

One more thing must be said about the null hypothesis. Rejecting it does not prove our alternate hypothesis, just suggests that random chance is not the most likely driver for the outcome. If you reject the null hypothesis because you have a certain number of boys, you can't immediatly accept that boys are all you can have. 

## Calculating The Odds

Once you've formulated your null hypothesis, how do you go about calculating the result? For our simple case, where we can define binary outcomes (boy/girl, heads/tails, pass/fail, etc...), and we're interested in calculating the probability of an outcome that meets a specified criteria (5 girls out of 5 children), the binomial distribution allows us to calculate this directly. 

The binomial distribution is given by: 

$\mathrm{P}(X=k) = \binom{n}{k} p^k(1-p)^{n-k}$ 

where $k$ is equal to the number of success (in this case the number of boys), $n$ is the number of [Bernoulli trials](https://www.youtube.com/watch?v=vT-n8dbXJoU) (in this case, the number of children) and $p$ is the probability of boys or girls (our null hypothesis puts this at 50%). Our case is a simple version of the above,  where $k = n$, and $p = 1-p = 0.5)$ so the equation above simplifies to:

$\mathrm{P}(X=k| k = n, p = 0.5) = \ p^k$ 

Lets calculate this probability for families with 1-10 children of the same sex, and see where the cutoff is:

```{r, message=F, warning=F}
library(tidyverse)
data_frame(numChildren = 1:10) %>%
  mutate(probabilityAllBoys = 0.5^numChildren) %>%
  ggplot(aes(numChildren,probabilityAllBoys)) +
  geom_point() + 
  geom_text(aes(label = round(probabilityAllBoys,3)*100),nudge_y = .015,nudge_x = -0.2) + 
  geom_hline(yintercept=.05, linetype="dashed", color = "red") + 
  scale_y_continuous(labels = scales::percent) + 
  ggtitle("Probability That All Children Will be Boys By Family Size")
```
This chart shows that if the probability of boy and girl are equal, and you sampled 100 families with 5 children, you would expect ~3 of them to have all boys! 

Often I encounter problems that are complicated enough that analytical solution (like the one above) is difficult or impossible. In this case we can employ my favorite method for doing hypothesis testing: Simulation.

## A Simple Simulation

Lets say we didn't know (or were too lazy) to calculate the equation above. We could imagine a set of pretend families who had pretend children in a way that aligns to the null hypothesis (50/50 chance of boy/girl), and figure out how often those families had all boys. 

![100 Families](/img/100Families.gif)

(OK so this is not a pretend family and they are all the same, but I'm sure you can use your imagination...)  
In R we can use sample to draw from 0s or 1s (boy or girl) with equal probability
```{r}
set.seed(1)
numberOfChildren = 5
probabilityOfboy = 0.5

oneFamily = sample(c(0,1),numberOfChildren,probabilityOfboy) #simulate one family
oneFamily
```
Our first family with 5 children had two girls (the 0's), then two boys(the 1's), then another girl. Our hypothesis just pertains to the number of boys, so we can just take the sum of our family:
```{r}
sum(oneFamily)
```
And we have the number of boys for this family.

To get the p value from our plot above, we need to do this procedure for 10,000 or so pretend families, and then calculate the percentage of those trials that match our observation (sum to a total of 5).

It turns out R gives us a shortcut to this, allowing us to get the number of boys posotive outcomes from successive Bernoulli trials quite easily:

```{r}
set.seed(1)
numberOfChildren =5
numberOfFamilies = 1
probabilityOfBoy = 0.5
rbinom(n=numberOfFamilies, size = numberOfChildren, prob = probabilityOfBoy)
```
The 2 here represents the same 2 that we got when we did sum(oneFamily)... Our first family has 2 boys. Lets do it now for 10,000 families:
```{r}
set.seed(1)
numberOfChildren =5
numberOfFamilies = 10000
probabilityOfBoy = 0.5
tenkFamilies = rbinom(n=numberOfFamilies, size = numberOfChildren, prob = probabilityOfBoy)
print(paste0("First Family: ", tenkFamilies[1], " Boys, 5010th Family: ", tenkFamilies[5010]," Boys"))
```
How often do we get all boys?
```{r}
mean(tenkFamilies == 5)
```
3.2% is close to the number we arrived at above (3.125) but not exactly. There are two reasons for this.   
**First**, because we are simulating randomness, the outcome of any given experiment (in this case 10000 families) will drift around 3.125, but won't be precisely 3.125. You can reduce [the standard error](https://www.youtube.com/watch?v=BwYj69LAQOI) in the estimate by sampling more families (try 100000000), with error going to zero as numberOfFamilies approaches infinity.  
**Second**, even with more samples, there is likely to be some error in our estimate due to the fact that the random number generator on our computer is not 'truly' random. It is a reasonable approximation of randomness, but it turns out randomness is not so easy to generate.

The threshold typically held for rejecting the null hypothesis is 5%. This suggests that if your family with 5 children has only boys, **you can statistically reject the null hypothesis and conclude that this result is not due to random chance**! Well, not so fast...

## Some Problems With Our Conclusion
While it is nice to have a conclusive line drawn in the sand about statistical significance, there are several problems with this conclusion. First, note that even with random chance, a non-trivial number of families out of 100 are likely to have *all girls* or *all boys*. Given the number of families with children, we must expect at least *some* of them will have children of only one sex. We could consider the entire distribution of families and discover if it deviates from our expectation, but drawing conclusions based on one family seems difficult in this case

It turns out that this issue has importance beyond simple curiosity. A p-value of 0.05 has been used as the standard across many scientific disciplines as a threshold for declaring a finding statistically significant and therefore publishable. Setting the bar too low means that we might not have reproduceable results (in other words, some papers will print results that aren't actually true). Setting the bar too high means that important results won't be able to be published.  

It would appear that the [evidence](https://www.nature.com/news/1-500-scientists-lift-the-lid-on-reproducibility-1.19970) is coming out on the side that the 0.05 threshold is too low, and thus, there has been a [push recently](https://jamanetwork.com/journals/jama/article-abstract/2676503) to lower the threshold for the p value from 0.05 to 0.005, or 0.5%. Partially derivative did a nice podcast on this, check it out [here](http://partiallyderivative.com/podcast/2017/08/08/p-values).  

## P Hacking
To get a sense of why this may be a problem we can do a simple thought experiment. The scientific process typically goes something like this: 

1. Come up with a hypothesis
  * X chemical will produce Y result...
2. Design and execute an experiment to test the hypothesis
  * X chemical had Y result with p statistical certainty.
3. If the result was successful (p-value<0.05), write a fancy paper to tell other people.

The formulation of the p-value is purposefully skeptical. It says, "OK, your trying to prove that this or that happens. What if nothing really is going on, how many times would we magically arrive at the conclusion you just observed?". Lets say we wanted to make the argument that the actual ratio of boys/girls for some populations is not 50/50. To do that we decided to study the distribution of children from 100 families. Turns out if you look at enough groups of 100 families, you'd find one that you could use to support your hypothesis:

![Lots of Families](/img/manyFamilies.gif)

In practice, there are several ways to accidentally find that something is statstically significant. For example, you could simply repeat an experiment until a statistically significant finding is observed.  More practically, in the course of experimentation, you might be testing hundreds or thousands of hypothesis at once. 

Perhaps you want to study what is influencing the defect rate for a particular product. You collect several thousand variables gathered during the manufacturing process and build a statistical model that predicts the defect rate. In a sense, you are performing thousands and thousands of experiments. Because of this, its highly likely that some of the variables will have statistically significant (p<0.05) correlation to the outcome **even there is absolutely no correlation!**

This stresses why it is important to evaluate the outcome of these types of experiments with a subject matter expert. Is there some physical or process explaination as to why these are correlated? Does this lead you towards another hypothesis you could use to independently verify the result?

How many children do we need to meet the more stringent p value suggested by the literature? 

```{r, message=F, warning=F}
library(tidyverse)
data_frame(numChildren = 1:10) %>%
  mutate(probabilityAllBoys = 0.5^numChildren) %>%
  ggplot(aes(numChildren,probabilityAllBoys)) +
  geom_point() + 
  geom_text(aes(label = round(probabilityAllBoys,3)*100),nudge_y = .015,nudge_x = -0.2) + 
  geom_hline(yintercept=.005, linetype="dashed", color = "red") + 
  scale_y_continuous(labels = scales::percent) + 
  ggtitle("Probability That All Children Will be Boys By Family Size")
```

If you have 8 children, and they are all one sex, you might have a problem on your hands.

But have we formulated the problem properly? What we really care about is how many children would you have to have of *either sex* to conclude something is wrong. To calculate the probability of this, you double all of the probabilities above. Doing so lands us at 9 children. 

**But lets be honest, if you have 9 children, you already have enough problems to worry about!**

If you made it this far, hope you enjoyed reading, and stay tuned for my next blog post which will use the simluation principle developed here to statistically verify claims made in childrens stories with web scraping, nlp, and parrallelized simulation!
